# This workflow runs benchmarks and detects performance regressions
name: Benchmark

on:
  push:
    branches: [ "master" ]
  pull_request:
    branches: [ "master" ]

permissions:
  contents: read

jobs:
  benchmark:
    name: python-benchmark
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12"]
    steps:
      - uses: actions/checkout@v6

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ matrix.python-version }}

      - name: Restore uv cache
        uses: actions/cache@v5
        with:
          path: /tmp/.uv-cache
          key: uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
          restore-keys: |
            uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
            uv-${{ runner.os }}

      - name: Install the project
        run: uv sync --locked --all-extras --dev

      - name: Run benchmarks and generate report
        id: benchmark
        run: |
          # Run benchmarks and save results to a timestamped JSON file
          mkdir -p performance-reports/results
          timestamp=$(date +%Y%m%d_%H%M%S)
          report_file="performance-reports/results/benchmark-results-$timestamp.json"

          # Run benchmarks with pytest and capture output
          benchmark_output=$(uv run pytest benchmarks/ -v --durations=0 --no-header --capture=no 2>&1 | tee /dev/stderr)

          # Extract benchmark results (assuming JSON output format)
          echo "$benchmark_output" > $report_file

          # Set output for use in subsequent steps
          echo "report_file=$report_file" >> $GITHUB_OUTPUT

      - name: Compare against baseline
        run: |
          python -c "
          import json
          import os
          from pathlib import Path

          # Load current results
          with open('${{ steps.benchmark.outputs.report_file }}', 'r') as f:
              current_results = json.load(f)

          # Find the latest baseline file (sorted by modification time)
          baselines_dir = Path('performance-reports/baselines')
          if not baselines_dir.exists():
              print('No baseline directory found - skipping comparison')
              exit(0)

          # Get all JSON files in baselines directory
          json_baselines = list(baselines_dir.glob('**/*.json'))
          if not json_baselines:
              print('No baseline JSON files found - skipping comparison')
              exit(0)

          # Sort by modification time (newest first)
          json_baselines.sort(key=lambda x: x.stat().st_mtime, reverse=True)
          latest_baseline = json_baselines[0]

          # Load baseline results
          with open(latest_baseline, 'r') as f:
              baseline_results = json.load(f)

          # Compare performance metrics (assuming same structure in both files)
          regression_detected = False
          for benchmark_name, current_metrics in current_results.items():
              if benchmark_name not in baseline_results:
                  continue

              baseline_metrics = baseline_results[benchmark_name]

              # Check each metric for regression (example: checking 'time' or 'duration')
              for metric in ['time', 'duration']:
                  if metric in current_metrics and metric in baseline_metrics:
                      baseline_value = baseline_metrics[metric]
                      current_value = current_metrics[metric]

                      # Calculate percentage change
                      if baseline_value > 0:
                          percent_change = ((current_value - baseline_value) / baseline_value) * 100

                          # Fail if performance degrades by more than 5%
                          if percent_change > 5.0:
                              print(f'⚠️  Performance regression detected in {benchmark_name} ({metric}):')
                              print(f'   Baseline: {baseline_value:.4f}, Current: {current_value:.4f}')
                              print(f'   Change: +{percent_change:.2f}% (regression)')
                              regression_detected = True

          # Exit with failure if any regression was detected
          if regression_detected:
              exit(1)
          else:
              print('✓ No performance regressions detected')
          ""
